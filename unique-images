this is a very small design doc for the dedup feature of zigzag

basically, we don't want to have duplicate images in ZigZag. this is a problem which pests many other image sites. If you stay on them  long enough, you will see reposts. Our goal for zigzag is for this never to happen. A user should see a continuous stream of images he has not seen before. You might ask whether this is wise? A funny image is a funny image, and maybe right now the images are not good, but someone posts a good but old image, and it increases the quality. This is true. Technically, if one explores long enough, one will see the full set of images, including the old and good ones. In the future, there's going to be a personalized stream, working by the principle of a recommendation engine. So, the images one sees are the "best" for that user. So there won't be the risk of missing out on the good ones because of a predilection for new ones.

For starters, we want to do a simple deduping. We can get into more complex setups here, where we look at the content of the image, and catch things like resized images etc., but for starters, just ensuring that images from the three sources aren't replicated is good enough. We don't assume an "adversarial" context.

The strategy is simple.
First assume we work with artifacts which have only one photo, which is an image (which can consist of several tiles).
We will compute an image hash. The hash is made up of the width of the image and the hight of the image, divided by eps. We allow some error in the size of the images to occur. This would group images by dimensions basically. This processing should occur after resizing to the standard size. So only height would actually be the parameter of interest. This will result in several tens buckets basically. This is not so great, as we have a much larger number of images. So each bucket will contain many images. Pairwise comparisons would be difficult. We should also look at something else. We could look at the mean of the image. Basically the DC DCT coefficient. Quantize it into 16 buckets, and operate like that. We'd have O(1000) bucket. Maybe look at the first 4 DC DCT coefficients. And then quantize them in 16 buckets. This will be good enough.
Now, we'll only look at the first tile. This should be enough for our purposes. We should also work on the luminance component of the image.

Now, when we get a new artifact, we compute the hash for it. We look it up in a database of hashes. If it has already been seen, we do no further processing. Otherwise, we add it both to the hash database and to the rest of the process.
The only change that needs to happen here is to somehow remove the duplicate data at the code level. We could have this deep in the photo_save server. The decoder looks at the first tile. It computes the hash. If it is good, it carries on as usual. If not, it throws an "ImageAlreadyExists" exception with the info that is important. The explorer will catch this exception and basically not do any of the next flow. I think this is a reasonable approach.
We could then have a photo_dedup source tree for handling computing the hash. It will involve extra libraries such as scikit or somethhing to compute the dct of the image. But it would be a separate library. Otherwise we have to invert things and make the photo_save first compute images and then somebody else should save them, which kind of defeats the purpose. I mean ... photo_save doesn't only do photo saving. It does all the nice formatting required to get a nice structure. So this falls well withing its bounds. It is settled then.

Now, we've handled the case of images. How about vidoes?
First, photos and videos are different. Since we don't assume an adversairla context, we can say that people won't do a video which is a single image looped, or a video looped twice as another video. Just simple stuff.
We can apply the same basic principles from the previous hash. However, we must look at frames as well. I would not want to precisely look at the number of frames. Although this could be part of it. We could look at the first frame, but that is many times just some dark thing or not informative. We should look at a set of predetermined frames. Something like [1,5,10,15,25] or however many they have. From these, we compute the same sort of fingerprint.

Ok, so the hash until now looks something like:
- [IMAGE, HEIGHT % 5, DCT(1,1) % 16, DCT(1,2) % 16, DCT(2,1) % 16, DCT(2, 2) % 15]
- [VIDEO, HEIGHT % 5, [dct_1], [dct_6], ..., [dct_25]]

This way, images and videos will be treated separately.

OK, this should take care of duplicates at this level.

What about at the artifact with multiple photos level? The situation is more complicatd. We could look at the exact set of photos. But this would be too strict, cause I have definitely seen overlap in groups. I think the best approach is to remove duplicated images. And if we remove more than 75% of all photos, remove the full artifact as well.
There are two main types of albums: related and unrelated images. For related images, such as multi-panel comics, we'd expect the duplication to be total. For unrelated it is going to be partial, and this heuristic will apply. There's no issue in losing just one image.
We might still have some errors but yeah .. that's life.

Finally, the hash for an photo should be something like a stable md5 hash of all the data.
